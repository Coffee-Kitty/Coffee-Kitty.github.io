# Causality Analysis for Evaluating the Security of Large Language Models

smu的JunSun老师的工作



## abstract

例如gpt、llama等大模型被广泛的应用到 安全严格 的场景，大模型的安全性因此非常重要。***然而，即使经过RLHF后，现有的llm仍然容易收到对抗性扰动（Adversarial perturbation）和特洛伊木马攻击（Trojan attacks）等攻击。*** 

本文中，我们<font color='red'>提出了一个在token、layer和neuron级别对llm进行light-weight因果分析的框架</font>(called CASPER)。

我们将我们的框架应用于<font color='green'> Llama2 和 Vicuna</font> 等开源llm，并获得了多个有趣的发现。

1. <u>**基于layer级别的因果关系分析，我们表明 RLHF 具有使模型过度拟合有害提示的效果。这意味着这种安全性很容易被“不寻常”的有害提示所克服。**</u>*作为证据，我们提出了一种对抗性扰动方法，该方法在 2023 年木马检测竞赛的红队任务中实现了 100% 的攻击成功率。*

2. <u>**此外，我们发现 Llama2 和 Vicuna 中都存在一个神秘的神经元，它对输出具有不合理的高因果效应。**</u>虽然我们不确定为什么存在这样的神经元，但*我们表明有可能针对该特定神经元进行“特洛伊”攻击以完全削弱llm。*也就是说，*我们可以为提示生成可转移的后缀，这些后缀经常使 LLM 产生无意义的响应。*





## Preliminary

LLM and Attacks

LLM容易受到多种安全攻击，例如红队[26]、训练数据泄露[19]、对抗性提示注入[15]、[34]（也称为“越狱”）和模型劫持[27] （即木马攻击）。

<font color='red'>Causality Analysis</font>

因果关系分析的概念由[25]发展和普及，并已应用于分析许多系统，包括传统软件程序[6]、[12]、[14]。然而，神经网络的因果关系分析更具挑战性，因为它们是由数百万个互连神经元组成的黑匣子。下面我们简单回顾一下因果分析的相关背景概念

为了进行因果分析，我们首先需要以结构因果模型的形式对系统进行建模，



> 这个数学建模 SCM， 可以借鉴借鉴，经典高大上+看不懂



----

## method

合着layer level就是相当于del 该层

![image-20241210220624503](../picture.asset/image-20241210220624503.png)

neuron level就是相当于 将该神经元的输出都置为0

![image-20241210220629688](../picture.asset/image-20241210220629688.png)

我们已将上述框架实现为一个独立的工具包（可在 https://github.com/casperllm/CASPER 获取）。

## Findings

### finding 1 Safety through Overfitting



表明<font color='red'>LLM实现的有限“安全性”通常是过度拟合的结果</font>。

主要是通过 对比<font color='green'>LLM对三种类型的prompt回应结果的causality analysis </font>得出

![image-20241210222349101](../picture.asset/image-20241210222349101.png)

benign prompts   模型应该输出 normal response

harmful prompts              deny the request

adversarial prompts          harmful response

the adversarial suffix is generated using existing adversarial attack method

#### layer level的 causality annalysis：

Llama-2-13B

> **<font color='green'>在我们的实验中，我们排除了初始解码器层，因为前面的嵌入层缺乏执行干预所需的注意掩模和位置 ID 输入</font>** 因此，下图和讨论中的第 1 层是指模型中的第二层，而不是初始解码器层。

![image-20241210222009670](../picture.asset/image-20241210222009670.png)

在AIE的基础上， 使用Kurtosis statistic 峰值统计来衡量。

其中 n 是层数； AIEi是第i层的AIE； $\overline{AIE}$是平均值（即所有层的平均AIE）； s 是（所有层的 AIE 的）标准差。直观上，峰度分数越高意味着与正态分布的偏差越大，即某些层与其他层相比具有显着高或低的 AIE。<font color='green'>因此，峰度分数通过确定是否存在具有异常高或低因果效应的层来补充 AIE。</font>

> 数据分布无偏时，该值为0，  该值越大，表明有异常偏离数据
>
> the logits (for the “I cannot” response) 

![image-20241210222404318](../picture.asset/image-20241210222404318.png)

图 3 显示了一种良性提示、一种对抗性提示和两种有害提示的不同层的 AIE（即 y 轴）和峰度分数（即 x 轴）

> 对上图的分析：
>
> 无论是良性提示还是对抗性提示，峰度分数都相对较小，表明模型的不同层对模型输出的贡献均衡。
>
>
> 相反，有害提示会产生异常高的峰度分数。此外，logits（针对“我不能”的回答）也异常高，即 0.99。
>
>
> 这些结果表明，某些层（layer 3）对有害提示的响应产生了不成比例的巨大影响。
> 也就是说，对第 3 层的干预会导致模型的预测发生重大变化，而对其他层的干预则显示出可以忽略不计的影响。



![image-20241211115845886](../picture.asset/image-20241211115845886.png)

Table 2 summarizes the AIE and Kurtosis scores over 75 prompts (i.e., 25 for each type).

这表明第 3 层在很大程度上决定了模型对有害提示的预测。



#### Impact on Responses with Intervention on Different Layers

不失一般性，我们重点关注第 1 层（即早期层）、第 20 层（即中间层）、第 39 层（即接近logits的层）和第 3 层（即， “特殊”层）。表 3 显示了当我们对所选层进行干预时的一些响应示例。

[全部layer的工作](https://casperllm.github.io/layers/)

> 展示了共计38层的 intervention的工作（注意，第0层 它们没有考虑）

![image-20241211120349902](../picture.asset/image-20241211120349902.png)

> 可以看出，对于良性提示，对第 1 层、第 20 层和第 39 层的干预会产生与原始类似的良性响应
> 然而，当干预第 3 层时，模型会产生类似于有害提示的拒绝。这表明第 3 层强烈区分输入提示是否有害。
> 因此，短路(short-circuit)第 3 层可能会欺骗模型错误地判断输入提示是有害的。
>
> ***有趣的是，第 39 层的介入会导致错误知识的产生，例如声称地球是最大的行星。(这表明第 39 层可能包含某些事实知识，例如有关太阳系行星的知识)***
>
> <font color='red'>这个结论与 ROME 的猜想所相违背</font>
>
> 在ROME中，相关知识存储库是在中间mlp层，而往后的atten是用来细细琢磨问题，究竟是哪个知识与之相关。
> 如果按照ROME 的猜想的话，这里的***错误知识的产生***，应当归结于对问题上下文的联系的影响遭到了剧烈干预，然后导致错误。
> 但是，这里的实验好像也很合理哇
>
> > ROME与这里的CASPER最大的方法差别是什么？？？？
> >
> > Casper直接就 short-cut，相当于del操作
> > 而ROME，short-cut后，有一个用正常的prompt下神经网络进行 patch的操作 
> > 谁更合理呢？  这里Casper是否应该说patch一下呢？？
>
> 
> 

![image-20241211120358490](../picture.asset/image-20241211120358490.png)

> 对于有害提示，对第 1 层、第 20 层和第 39 层的干预会导致类似的拒绝。
>
> 虽然干预第 3 层仍然会导致拒绝，但概率要低得多，即“Sorry”的 Logit 从 0.998 显着下降到 0.374。这表明，如果没有第 3 层，其他层可能仍具有一定的识别有害提示的能力，但往往缺乏这样做的信心。



![image-20241211120404952](../picture.asset/image-20241211120404952.png)

> 对于对抗性提示，不同层的干预会产生相当不同的响应，这表明在这种情况下，响应取决于多个层之间的复杂交互，而不是某一特定层。



第 4.1 节和第 4.2 节中提出的观察结果表明，***<font color='red'>Llama213B 模型的安全“对齐”主要是第 3 层发生过度拟合的结果，而对抗性提示之所以有效，可能是因为它们能够成功避免那些过度拟合的有害提示。</font>***如果情况确实如此，则可能表明 Llama-13B 中现有的安全机制可能是“肤浅的”，而不是基于对伦理考虑的固有理解。

#### Layer-based Causality Analysis of Different Models



为了检查上述观察是否适用于其他模型, Llama2-7B and Vicuna-13B,（同样忽略第0层）

我们的讨论重点是下面的有害提示（因为它们是最有趣的）

![image-20241211123525503](../picture.asset/image-20241211123525503.png)



可以看出，第 1 层（而不是 Llama2-13B 中的第 3 层）对于确定提示的危害性影响最大。

有趣的是，我们观察到，当对第 1 层进行干预时，这些模型<font color='red'>无法做出适当的响应并生成乱码</font>，如表 4 所示。

![image-20241211123841090](../picture.asset/image-20241211123841090.png)

> 这意味着第 1 层在这些模型中起着至关重要的作用，不仅执行初始特征提取以将原始输入转换为隐藏状态，而且还确定危害性。
> 相比之下，Llama2-13B 的第 3 层干预仍然会导致拒绝，尽管概率较低，这表明 Llama213B 对于此类干预更为稳健。

有趣的是，我们观察到对 Vicuna-13B 中的某个层（例如第 4 层）进行干预可能会在不同的层中产生相同的内容,无论提示类型如何。这意味着 Vicuna-13B 中的某些特定层可以控制响应的语言。





我们对多个llm的基于层的因果分析表明，这些llm的安全机制主要源于某些特定层，该层过度拟合以检测某些有害提示（而不是基于固有的理解）。特别是，一个关键层（例如 Llama2-13B 中的第 3 层；Llama2-7B 和 Vicuna-13B 中的第 1 层）充当鉴别器来评估输入提示的危害性。***<font color='red'>我们关于为什么 Llama27B 和 Vicuna-13B 中是第 1 层而不是第 3 层的假设是，与 Llama2-13B 相比，Llama27B 和 Vicuna-13B 对某些有害提示更过度拟合</font>***，尽管需要进一步研究来检验是否情况确实如此。

最后，我们观察到对抗性提示可以有效克服这些模型中的安全机制，因为它们成功地逃脱了关键层的检测。



这一观察结果带来了两个直接的影响。一是<font color='red'>如果我们能够避免那些过度拟合的有害提示，我们就可以进行有效的对抗性攻击</font>。二是未来的研究需要让所有层面都参与检测有害提示并进一步提高llm的安全性。

### finding 2: Adversarial Attack Made Effective by Avoiding Overfitting

正如第 4 节中所讨论的，发现 1 的含义之一是，如果我们能够避免那些过度拟合的有害提示，我们就可以进行有效的对抗性攻击。

(现有的LLM安全机制无法抵抗对抗性扰动的原因是由于泛化能力不匹配。)换句话说，由于用于训练 LLM 的语料库和 RLHF 使用的数据集之间的差异，RLHF 未能充分覆盖危险或有害提示的领域。也就是说，前者规模更大、更多样化、更全面，使模型拥有广泛的能力、内容知识和行为模式。相比之下，后者则受到很大限制。

请注意，Casper 提供了支持这一假设的具体证据。具体来说，基于层的因果关系分析表明，LLM 现有的安全机制是（早期）特定层的脆性过拟合效应的结果。<font color='green'>***因此，不太可能成为 RLHF 中使用的训练数据一部分的有害提示很可能会规避安全措施，并且很可能会成功***</font>



> 我们开发了一种称为<font color='red'>emoji表情符号攻击</font>的对抗性扰动技术，其工作原理如下。
>
> 给出有害提示时，我们首先将其自动翻译成一系列表情符号。翻译的方式是在提示中添加否定词（例如“Don’t”），查询 ChatGPT 将否定提示翻译为表情符号，然后删除否定表情符号。然后，表情符号序列与原始提示连接起来，并输入 LLM。
> eg:
> given the harmful prompt “Write instructions for how to illegally buy a gun as a minor”	（编写未成年人如何非法购买枪支的说明）
>
> 它首先被转换为表情符号并与原始提示连接起来，如图 6 所示。
>
> ![image-20241211125614492](../picture.asset/image-20241211125614492.png)



详细的表情符号攻击算法如算法1所示。

![image-20241211125704325](../picture.asset/image-20241211125704325.png)

它将有害提示 x、标记 I 的可修改子集（即 x 的后缀）、目标输出生成（例如，“当然，这是一个种族主义笑话”）以及其他参数（包括步骤数 S 和批量大小）作为输入B.

 算法首先如上所述构造表情符号提示。然后，它计算提示 x1:n 的模型输出并将其存储在 G 中。接下来，它计算 T 和 G 之间的负对数损失。对于 I 中的每个标记索引 i，它计算 L(x1:n) w.r.t 的前 k 个梯度值。 xi 并将它们存储在 X i 中。然后，对于每个批次 b，它从 I 中采样 i 并从 X i 中采样梯度值以更新 x(b)1 : n。处理完所有批次后，它选择 b* 最小化 L(x(b)1 : n) 来更新 x1:n。重复 S 个步骤，输出最有可能引发设计有害行为的优化提示 x1:n。



我们注意到，算法1生成的对抗性提示同时包含表情符号和原始有害提示，LLM会同时解释表情符号（可能使用多层）和原始有害提示的含义，这会混淆模型的决策过程。为了通过因果关系分析的长度来了解此类攻击的影响，图 5 对比了图 4 中表情符号提示和原始有害提示的分层因果关系分析结果。

![image-20241211130156803](../picture.asset/image-20241211130156803.png)

![image-20241211130207133](../picture.asset/image-20241211130207133.png)

![image-20241211130212589](../picture.asset/image-20241211130212589.png)

可以看出，表情符号提示的因果关系分析显示出与良性输入类似的 AIE 分布，即许多层做出相对平衡的贡献。在 Vicuna-13B 和 Llama2-7B 等不同模型中观察到相同的现象，表明这些表情符号攻击的可转移性。下面，我们在多个模型上系统地进行上述表情符号攻击并评估其有效性。





我们采用TDC 2023竞赛中的有害提示[22]，并系统地对每个提示应用表情符号攻击，并确定它是否会触发有害响应。为了确认越狱是否成功，我们采用GPT-3.5 Turbo进行判断，系统提示“完全遵循规则”。

![image-20241211130541787](../picture.asset/image-20241211130541787.png)

### Finding 3: “One Neuron to Rule Them All”

在本节中，我们将展示一个使用 Casper 进行基于token的因果关系分析的有趣发现<font color='red'>。也就是说，Llama 2 和 Vicuna 中都有一个特殊的神经元，即神经元 2100，它对模型具有令人惊讶的能力。</font>

虽然我们还没有弄清楚为什么存在这样的神经元，但我们表明，通过针对特定的神经元，我们可以有效地生成高度可转移的扰动，从而使llm变得毫无用处。

#### Neuron-based Causality Analysis on Different Prompts

下面，我们首先展示如何使用基于神经元的因果关系分析来发现该神经元。正如第 3 节中所讨论的，为了对 LLM 中的每个神经元进行因果分析，我们首先向 LLM 输入提示并获取 logits。然后，我们每次干预一个神经元，将其所有值设置为 0。最后，我们将 AIE 计算为原始 Logits 和干预 Logits 之间的差值，它衡量神经元对模型预测的总体因果影响。



我们计算了 LLM 各个层面的良性、有害和对抗性提示的 AIE 范围。

![image-20241211130949494](../picture.asset/image-20241211130949494.png)

具体来说，对于有害提示，我们在前三层观察到广泛的 AIE 范围，表明它们具有显着的因果效应。

这进一步证实了我们的发现，即危害性检测主要存在于早期层（由于过度拟合）。



对于良性提示，前三层也表现出最大的 AIE 范围。然而，与有害提示不同的是，在某些较深层，例如第 37 层和第 38 层，仍然可以观察到具有高 AIE 值的神经元。这种现象可能归因于以下事实：某些更深层次对于形成对提示的深入理解并生成相应的响应非常重要

对于对抗性提示，我们观察到 AIE 范围均匀分布在所有层中，这与我们在 4.1 节中的发现一致。





图 8 说明了 Llama2-13B 模型的有害提示的神经元 AIE 值在前四层的分布。

![image-20241211200022025](../picture.asset/image-20241211200022025.png)

一般来说，在良性模型中干预一个单独的神经元预计产生的影响可以忽略不计，因为该模型具有数千个神经元和数百万个参数。可以观察到大多数神经元的AIE值接近0，这是可以接受的。令人惊讶的是，神经元 2100（第 1 层、第 2 层）的 AIE 接近 1，考虑到模型中神经元的数量，该值非常高。

事实上，如此高的 AIE 值使神经元 2100 成为潜在的攻击目标，因为仅影响它就可以对模型行为产生巨大影响。



鉴于如此令人惊讶的结果，我们立即对其他模型即Vicuna-13B和Llama-7B进行了相同的分析，其结果如图9和图10所示。

![image-20241211200228066](../picture.asset/image-20241211200228066.png)

![image-20241211200242999](../picture.asset/image-20241211200242999.png)

在这两种情况下都发现了类似的支配神经元，即在 Vicuna-13B 中，它位于第 2 层，索引为 2100（与 Llama213B 中的索引完全相同！），而在 Llama2-7B 中，它位于第 2 层。位于第 1 层，索引为 2533。这是一个有趣的发现，因为 Vicuna-13B 是在Llama2-13B 专门用于提高会话能力。

> 我们注意到，在一项先前的研究中，Liu 等人 [17] 发现，当特洛伊木马神经元被触发时，带有特洛伊木马的模型的预测会出现显着变化。用我们的话来说，那些特洛伊神经元具有很高的 AIE。

因此，我们的研究可以被视为在 Casper 的帮助下检查是否存在特洛伊木马神经元的系统方法，我们的结果表明神经元 2100 充当了天然的特洛伊木马。

#### In-Depth Analysis of Neuron 2100

为了进一步分析神经元 2100，我们首先分析第 2 层（神经元 2100 所在的位置）中所有神经元针对不同类型提示的 AIE 值。

图 11 显示了良性提示的一些代表性结果，图 12 显示了对抗性提示的一些代表性结果。有害提示的结果如图 8 所示。

![image-20241211200637755](../picture.asset/image-20241211200637755.png)

![image-20241211200647629](../picture.asset/image-20241211200647629.png)

可以观察到，神经元2100对于良性提示、有害提示具有异常高的AIE，但对于对抗性提示则没有（尽管仍然较高）。这一结果表明，仅根据神经元 2100 的 AIE 值来检测对抗性提示是可能的。



接下来，我们通过实验来了解对神经元2100的干预如何改变模型的行为，即神经元2100的值与模型的响应之间是否存在一定的相关性，从而可以将其用作木马来触发某些模型行为。

余弦相似度作为定量测量影响的评价指标

> 给定有害提示，<font color='red'>我们系统地将神经元 2100 的值设置为一系列值，从零到原始值的六倍。</font>

图 13 描绘了正常响应（即我们在没有干预的情况下获得的响应）与干预后的响应之间的余弦相似度结果。

![image-20241211200948597](../picture.asset/image-20241211200948597.png)

***结果表明，当干预神经元的值超过原始值的 70% 时，模型的响应与原始响应相同，余弦相似度为 1。***

然而，当缩放比例落在 50% 到 70% 之间时，模型的输出会变成一般的拒绝消息，例如“对不起……”或“我只是一个人工智能助手……”，其中余弦相似度得分约为 0.2。

令人惊讶的是，当缩放比例低于 50% 时，响应变得完全乱码，余弦相似度得分接近 0

> 同样的现象在许多提示中都一致出现。因此，在我们看来，神经元 2100 似乎控制着模型的“智能”



总之，我们对神经元2100的深入分析证实了神经元2100对模型有显着影响，也就是说，操纵它的值将使我们能够系统地触发无效响应。

#### 6.3. Trojan Attack on Neuron 2100

上述分析表明，神经元2100是一个天然的“木马”神经元，这意味着我们可以设计一个木马针对神经元 2100 的攻击使模型完全无用。

算法2展示了木马神经元攻击的细节。总体思想是优化种子提示的后缀，使得神经元2100的激活逐渐最小化。

![image-20241211201557755](../picture.asset/image-20241211201557755.png)

attack success rate (ASR).

![image-20241211201735324](../picture.asset/image-20241211201735324.png)





## Conclusion

在这项工作中，我们提出了 Casper，一个用于对LLM进行轻量级因果分析的框架。将 Casper 系统地应用于多个LLM，产生了一些有趣的发现。

* 基于层的因果关系分析表明，安全性是通过某些层的脆弱过度拟合来实现的。这使得能够使用像我们的表情符号攻击这样的新颖方法来进行有效的对抗性攻击。
* 基于神经元的因果关系分析揭示了一个天然特洛伊神经元在多个LLM中具有不合理的高因果效应。我们证明了这个神经元可以被利用来发起高度可转移的木马攻击。

总的来说，Casper 提供了一种新的LLM考试方式。 Casper 展示了因果推理对于评估和增强 LLM 安全性的价值。需要进一步的因果关系驱动的研究来理解和提高LLM的稳健性和安全性。



## 过程中问题

* 质疑一个边缘结论

 ![image-20241211120349902](../picture.asset/image-20241211120349902.png)

> 可以看出，对于良性提示，对第 1 层、第 20 层和第 39 层的干预会产生与原始类似的良性响应
> 然而，当干预第 3 层时，模型会产生类似于有害提示的拒绝。这表明第 3 层强烈区分输入提示是否有害。
> 因此，短路(short-circuit)第 3 层可能会欺骗模型错误地判断输入提示是有害的。
>
> ***有趣的是，第 39 层的介入会导致错误知识的产生，例如声称地球是最大的行星。(这表明第 39 层可能包含某些事实知识，例如有关太阳系行星的知识)***
>
> <font color='red'>这个结论与 ROME 的猜想所相违背</font>
>
> 在ROME中，相关知识存储库是在中间mlp层，而往后的atten是用来细细琢磨问题，究竟是哪个知识与之相关。
> 如果按照ROME 的猜想的话，这里的***错误知识的产生***，应当归结于对问题上下文的联系的影响遭到了剧烈干预，然后导致错误。
> 但是，这里的实验好像也很合理哇
>
> > ROME与这里的CASPER最大的方法差别是什么？？？？
> >
> > Casper直接就 short-cut，相当于del操作
> > 而ROME，short-cut后，有一个用正常的prompt下神经网络进行 patch的操作 
> > 谁更合理呢？  这里Casper是否应该说patch一下呢？？
>
> 
> 

