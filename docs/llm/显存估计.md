<!--
 * @Author: coffeecat
 * @Date: 2025-03-31 20:19:29
 * @LastEditors: Do not edit
 * @LastEditTime: 2025-03-31 20:22:58
-->
# 显存估计
## 关于训练显存估计错误的修正

配合人大高领的llm book来看，但注意，可能有误


训练显存估算如下，假设为DeepSeek-R1-Distill-Qwen-1.5B，fp16计算
1B约占1G显存
### 1.模型：  模型参数1.5B个字节 ，模型梯度 1.5B， 即2* 16/8*1.5=4*1.5B 
### 2.优化器 ： Adam 
以DeepSeek-R1-Distill-Qwen-1.5B来看，占2* 32/8 * 1.5B = 8*1.5B 
供12*1.5B=16.8G

https://www.bilibili.com/video/BV1NZ421s75D/?spm_id_from=333.337.search-card.all.click&vd_source=39767bfbc4ae772d0c2f8d8b32b54ce6

Adam结合了Momentum和RMSProp的思想，
前者是为了解决随机梯度下降时，梯度值大的参数上容易训练震荡的问题，
利用历史梯度的指数加权平均值，并做修正

后者则是训练时有的梯度很大，有的梯度很小，所以导致训练震荡
所以对每个参数除以过去累计梯度的平方，这样每个梯度就差不多大

总之一句话，nn的参数上，有的参数梯度值很大，有的很小，结合了momentum动量，用历史梯度值的指数加权平均并做修正，以及RMSProp，利用历史累计梯度值的平方对当前梯度做归一化（近似看待）的思想。从而缓解训练问题。

Adam和AdamW中，
每个nn参数的位置，都需要保存两个值，一个是历史梯度的指数加权平均，
一个是历史梯度平方的指数加权平均，然后做了上述两个思想融合的神奇操作。

**由于梯度值一般很小，所以要还得用fp32**




### 3.训练数据： 请一定注意，这一项也不可以忽略，占比很大！！！
以DeepSeek-R1-Distill-Qwen-1.5B来看，
考虑不使用张量并行、流水线并行、激活重计算等优化方法时，
约占  L=28,B设置为1，seq为128k，hidden为1536,
则使用flash约为 28(22* 1* 128k* 1536 )=28(22*128k*1.5k)=115.5B= 128*0.9B
不使用flash的增量约为  28* 2 * 1 * 128k*128k*fp16/8 = 128*128*9.1G



大头就是多个Decoder-only层，假设有L层，输入为(bs,seq,hidden)
则在MHA部分，
首先是一份输入BSH,然后是QKV运算后的结果，3BSH,

若未使用 FlashAttention 优化,  公式 5.5 中 QK⊺ 的结果也需要保存,占用 SSH 字节;若使用了 FlashAttention,  则无需此部分开销。



### 4.代码库，训练中间结果，显存碎片等显存占用

下图为bs=1,seq=512的显存占用情况，
Fp16, DeepSeek-R1-Distill-Qwen-1.5B，
当使用 SFTTrainer 时，它默认使用的优化器是 AdamW

基本预测分析为
16.8G 
0.5\*0.9/1.024/1.024/1.024 + 0.5*0.5*9.1=0.4+2.275=  2.7G

综上，16.8+2.7=19.5G
最后框架占显存，pytorch0.8G-1G
由上图可知，大致正确

验证关于flash_attention的计算，
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            torch_dtype=torch.float16,
            use_flash_attention_2=True  # 启用 Flash Attention        
)

其中，关于flash_attention，实际为19042-18960大概 100MB  
这还是不太对。。


##  peft下的显存估计
那么关于，QwQ-32B，fp16， lora微调all-linear,
12Pa+2P=12\*0.9+2\*32=74.8

数据最大占比将为 
考虑不使用张量并行、流水线并行、激活重计算等优化方法时，
约占  L=64,B设置为1，seq为1k，hidden为5120,
则使用flash约为 64(22* 1* 1k* 5120 )=6.8G
不使用flash的增量约为  64* 2 * 1 * 1k\*1k\*fp16/8 = 0.25G

综上，将约为80G左右

### 新增参数量计算
```python
'''
Author: coffeecat
Date: 2025-03-28 01:47:27
LastEditors: Do not edit
LastEditTime: 2025-03-31 01:47:27
'''
from transformers import AutoModelForCausalLM
from peft import LoraConfig, get_peft_model

# 加载原始模型
model_name = "../QwQ-32B"  # 替换为你实际使用的模型名称
model = AutoModelForCausalLM.from_pretrained(model_name)
# 计算原始模型参数量
P = sum(p.numel() for p in model.parameters())

# # 配置 LoRA
# lora_config = LoraConfig(
#     r=8,
#     lora_alpha=16,
#     target_modules=["q_proj", "v_proj"],
#     lora_dropout=0.05,
#     bias="none",
#     task_type="CAUSAL_LM"
# )
lora_config = LoraConfig(
    r=16,
    lora_alpha=16,
    lora_dropout=0.05,
    bias="none",
    target_modules=[
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj"
    ],
    modules_to_save=["lm_head", "embed_token"],
    task_type="CAUSAL_LM",
)
# 获取 LoRA 模型
lora_model = get_peft_model(model, lora_config)
# 计算 LoRA 增加的参数量
P_a = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)

print(f"原始模型参数量 P: {P}")
print(f"LoRA 增加的参数量 P_a: {P_a}")

"""
原始模型参数量 P: 32763876352
LoRA 增加的参数量 P_a: 912785408
"""

```
Fp16下训练参数估计即12Pa+2P字节
### 注意一定要冻结原模型参数


***注意一定要冻结原模型参数***
需要显示指定优化器只去优化新增的AB矩阵部分，不然优化器默认又是整体模型
```python

def train_model(train_dataset):
    """
    训练模型
    :param train_dataset: 训练数据集
    """
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
        data_collator = prepare_data_collator(tokenizer)
        
        # 加载模型并启用LoRA
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            torch_dtype=torch.float16,
            # use_flash_attention_2=True
            attn_implementation="flash_attention_2",
        )
        
        # 冻结原模型参数
        model.requires_grad_(False)
        
        # 应用LoRA配置
        peft_config = LoraConfig(
            r=16,
            lora_alpha=16,
            lora_dropout=0.05,
            bias="none",
            target_modules=[
                "q_proj",
                "k_proj",
                "v_proj",
                "o_proj",
                "gate_proj",
                "up_proj",
                "down_proj"
            ],
            modules_to_save=["lm_head", "embed_token"],
            task_type="CAUSAL_LM",
        )
        
        # 初始化LoRA参数
        model = get_peft_model(model, peft_config)
        
        # 验证可训练参数
        for name, param in model.named_parameters():
            if param.requires_grad:
                print(f"Trainable parameter: {name}")
        
        # 配置训练参数
        training_args = SFTConfig(
            output_dir=OUTPUT_DIR,
            report_to="tensorboard",
            num_train_epochs=1,
            per_device_train_batch_size=1,
            logging_steps=1,
            save_steps=164,
            fp16=True,  # 启用混合精度
            gradient_checkpointing=True,  # 激活重计算
            optim="adamw_torch_fused"  # 使用优化的AdamW
        )
        
        # 创建仅包含LoRA参数的优化器
        optimizer = torch.optim.AdamW(
            [p for p in model.parameters() if p.requires_grad],
            lr=1e-4
        )
        
        # 创建训练器
        trainer = SFTTrainer(
            model,
            train_dataset=train_dataset,
            data_collator=data_collator,
            args=training_args,
            optimizers=(optimizer, None)  # 传递自定义优化器
        )
        
        accelerator = Accelerator()
        model, trainer = accelerator.prepare(model, trainer)
        
        try:
            trainer.train()
        except Exception as e:
            logging.error(f"Training failed: {e}")
    except Exception as e:
        print(f"模型训练出错: {e}")



```
