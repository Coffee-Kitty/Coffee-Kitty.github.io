<!--
 * @Author: coffeecat
 * @Date: 2025-03-10 10:35:34
 * @LastEditors: Do not edit
 * @LastEditTime: 2025-03-11 16:18:31
-->

# openmanus


## ç¯å¢ƒ

ç…§ç€ç½‘å€ https://github.com/Coffee-Kitty/OpenManus æ¥å°±å¯ä»¥


### å·¥å…·å®‰è£…
æ¥æºäºè¿è¡Œä¸­æ¨¡å‹æŠ¥é”™
playwright install  

è²Œä¼¼ç”±äºdockeråŸå› ï¼Œ æ— å¤´æµè§ˆå™¨éš¾ä»¥åº”ç”¨ï¼Œ agenté‡‡ç”¨çš„æ›¿ä»£æ‰‹æ®µæ˜¯requestsåº“


### è¿è¡Œ
è¿è¡Œpyhton main.py

prmopt = å¯¹Karpathyçš„ç½‘ç«™ï¼ˆhttps://karpathy.ai/ï¼‰è¿›è¡Œå…¨é¢çš„SEOå®¡æ ¸ï¼Œå¹¶æä¾›è¯¦ç»†çš„ä¼˜åŒ–æŠ¥å‘Šï¼ŒåŒ…æ‹¬å¯æ‰§è¡Œçš„æ”¹è¿›å»ºè®®ã€‚


```bash
2025-03-10 03:53:25.075 | INFO     | app.agent.base:run:137 - Executing step 1/30
2025-03-10 03:53:37.813 | INFO     | app.agent.toolcall:think:53 - âœ¨ Manus's thoughts: ä¸ºäº†å¯¹Andrej Karpathyçš„ç½‘ç«™ï¼ˆhttps://karpathy.ai/ï¼‰è¿›è¡Œå…¨é¢çš„SEOå®¡æ ¸ï¼Œæˆ‘ä»¬å°†æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š

1. ä½¿ç”¨æµè§ˆå™¨å·¥å…·è®¿é—®ç½‘ç«™å¹¶è·å–é¡µé¢çš„å†…å®¹å’ŒHTMLä»£ç ã€‚
2. æ£€æŸ¥ç½‘ç«™çš„åŸºç¡€SEOå…ƒç´ ï¼Œå¦‚æ ‡é¢˜æ ‡ç­¾ã€å…ƒæè¿°ã€å…³é”®è¯ä½¿ç”¨æƒ…å†µã€å›¾ç‰‡ALTå±æ€§ç­‰ã€‚
3. åˆ†æç½‘ç«™çš„åŠ è½½é€Ÿåº¦ã€‚
4. æŸ¥çœ‹ç½‘ç«™çš„ç§»åŠ¨è®¾å¤‡å‹å¥½æ€§ã€‚
5. æ£€æŸ¥ç½‘ç«™çš„ç»“æ„å’Œå†…éƒ¨é“¾æ¥ã€‚
6. åˆ†æç½‘ç«™çš„å¤–éƒ¨é“¾æ¥æ¦‚å†µã€‚
7. æä¾›ä¸€ä»½è¯¦ç»†çš„SEOä¼˜åŒ–æŠ¥å‘Šï¼ŒåŒ…æ‹¬ä¸Šè¿°æ£€æŸ¥çš„ç»“æœå’Œå…·ä½“çš„æ”¹è¿›å»ºè®®ã€‚
```

é¦–å…ˆï¼Œè®©æˆ‘ä»¬ä½¿ç”¨`browser_use`å·¥å…·å¯¼èˆªåˆ°Karpathyçš„ç½‘ç«™ï¼Œå¹¶è·å–å…¶HTMLå†…å®¹ã€‚


## ä»£ç åˆ†æ

æˆ‘è§‰å¾—çš„ä¸‰å¤§ä¼˜ç‚¹
1. å¼‚æ­¥æœºåˆ¶çš„åç¨‹
2. reactä¸tool usingçš„ç»“åˆ
3. ä¸°å¯Œçš„tool using

é¦–å…ˆæ˜¯æ–‡ä»¶ç›®å½•ç»“æ„
![alt text](assets/openmanus/image.png)


### config
ä¸»è¦æ˜¯åˆ©ç”¨config.pyè¯»å– config.example.tomlä¸­å†…å®¹ï¼Œä¸»è¦æ¶‰åŠllmçš„é…ç½®

å…³äºllmçš„é…ç½®ï¼Œå°è£…äº†ä¸€ä¸ªLLMSettingç±»
```python
from pydantic import BaseModel, Field
# è¿™é‡Œæ˜¯ç»§æ‰¿çš„pydanticåº“ä¸­çš„ç±»
class LLMSettings(BaseModel):
    model: str = Field(..., description="Model name")
    base_url: str = Field(..., description="API base URL")
    api_key: str = Field(..., description="API key")
    max_tokens: int = Field(4096, description="Maximum number of tokens per request")
    temperature: float = Field(1.0, description="Sampling temperature")
    api_type: str = Field(..., description="AzureOpenai or Openai")
    api_version: str = Field(..., description="Azure Openai version if AzureOpenai")
```
å…¶ä¸­å…³äºconfigçš„é…ç½®å€¼å¾—ä¸€æï¼Œåšäº†ä¸€ä¸ªå•ä¾‹æ¨¡å¼
```python

class Config:
    """
    å•ä¾‹æ¨¡å¼
    """
    _instance = None
    _lock = threading.Lock()
    _initialized = False

    """
åœ¨åˆ›å»ºå¯¹è±¡æ—¶ï¼ŒPython é¦–å…ˆè°ƒç”¨ __new__ æ–¹æ³•ã€‚
__new__ çš„è¿”å›å€¼ä¼šä¼ é€’ç»™ __init__ æ–¹æ³•ã€‚

 __new__ æ˜¯ä¸€ä¸ªé™æ€æ–¹æ³•ï¼ˆå°½ç®¡ä¸éœ€è¦æ˜¾å¼å£°æ˜ä¸º @staticmethodï¼‰ï¼Œç”¨äºåˆ›å»ºå¹¶è¿”å›ä¸€ä¸ªæ–°çš„å¯¹è±¡å®ä¾‹ã€‚
å®ƒæ˜¯å¯¹è±¡åˆ›å»ºçš„ç¬¬ä¸€æ­¥ï¼Œè´Ÿè´£åˆ†é…å†…å­˜å¹¶è¿”å›ä¸€ä¸ªæ–°å®ä¾‹
__init__ æ˜¯ä¸€ä¸ªå®ä¾‹æ–¹æ³•ï¼Œç”¨äºåˆå§‹åŒ–å¯¹è±¡ã€‚
å®ƒæ˜¯åœ¨å¯¹è±¡åˆ›å»ºåè°ƒç”¨çš„ï¼Œè´Ÿè´£è®¾ç½®å¯¹è±¡çš„åˆå§‹çŠ¶æ€ã€‚


    """
    def __new__(cls):
        # åŒæ£€é”ï¼ˆDouble-Checked Lockingï¼‰
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self):
        #åŒæ£€é”ï¼ˆDouble-Checked Lockingï¼‰
        if not self._initialized:
            with self._lock:
                if not self._initialized:
                    self._config = None
                    self._load_initial_config()
                    self._initialized = True

    @staticmethod
    def _get_config_path() -> Path:
        # ä¸»è¦è·å–é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œ ä¼šæ£€æŸ¥ config.tomlå’Œcofig.example.toml æ˜¯å¦å­˜åœ¨
        root = PROJECT_ROOT
        config_path = root / "config" / "config.toml"
        if config_path.exists():
            return config_path
        example_path = root / "config" / "config.example.toml"
        if example_path.exists():
            return example_path
        raise FileNotFoundError("No configuration file found in config directory")

    def _load_config(self) -> dict:
        # ä½¿ç”¨tomllibè¯»å–é…ç½®æ–‡ä»¶
        config_path = self._get_config_path()
        with config_path.open("rb") as f:
            return tomllib.load(f)

    def _load_initial_config(self):
        # è°ƒç”¨ä¸Šé¢_load_configï¼Œ 
        # å¹¶ä¸”è¿›è¡Œå°è£…
        raw_config = self._load_config()
        base_llm = raw_config.get("llm", {})
        llm_overrides = {
            k: v for k, v in raw_config.get("llm", {}).items() if isinstance(v, dict)
        }

        default_settings = {
            "model": base_llm.get("model"),
            "base_url": base_llm.get("base_url"),
            "api_key": base_llm.get("api_key"),
            "max_tokens": base_llm.get("max_tokens", 4096),
            "temperature": base_llm.get("temperature", 1.0),
            "api_type": base_llm.get("api_type", ""),
            "api_version": base_llm.get("api_version", ""),
        }

        config_dict = {
            "llm": {
                "default": default_settings,
                **{
                    name: {**default_settings, **override_config}
                    for name, override_config in llm_overrides.items()
                },
            }
        }

        self._config = AppConfig(**config_dict)

    @property
    def llm(self) -> Dict[str, LLMSettings]:
        return self._config.llm

# appconfig å³{k=åå­—ï¼Œv=LLMSettingç±»}
class AppConfig(BaseModel):
    llm: Dict[str, LLMSettings]

"""
[llm]
model = "qwen2.5-72b-instruct"
api_key = ""
base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
max_tokens = 4096
temperature = 0.0
[llm.vision]
model = "claude-3-5-sonnet"
base_url = "https://api.openai.com/v1"
api_key = "sk-..."

å³æœ€ç»ˆå°†ä¸Šè¿°é…ç½®ï¼Œå°è£…ä¸º
AppConfig{
llm: LLMSetting{model:... , api_key:..., ...},
vision: LLMSetting{... : ...}
}
"""
```


### log
logger.pyè§„èŒƒäº†ç¨‹åºè¿è¡ŒæœŸé—´çš„ æ§åˆ¶å°è¾“å‡º ä¸ æ–‡ä»¶æ—¥å¿—è®°å½•ã€‚
å…·ä½“å‚è§æ³¨é‡Š
```python
import sys
from datetime import datetime

from loguru import logger as _logger

from config import PROJECT_ROOT


_print_level = "INFO"


def define_log_level(print_level="INFO", logfile_level="DEBUG", name: str = None):
    """Adjust the log level to above level"""
    global _print_level
    _print_level = print_level

    current_date = datetime.now()
    formatted_date = current_date.strftime("%Y%m%d%H%M%S")
    log_name = (
        f"{name}_{formatted_date}" if name else formatted_date
    )  # name a log with prefix name

    _logger.remove()# ç§»é™¤ loguru é»˜è®¤çš„æ—¥å¿—å¤„ç†å™¨ã€‚
    _logger.add(sys.stderr, level=print_level) # æ·»åŠ ä¸€ä¸ªæ§åˆ¶å°æ—¥å¿—å¤„ç†å™¨ï¼Œæ—¥å¿—çº§åˆ«ä¸º print_levelã€‚
    _logger.add(PROJECT_ROOT / f"logs/{log_name}.log", level=logfile_level) # æ·»åŠ ä¸€ä¸ªæ–‡ä»¶æ—¥å¿—å¤„ç†å™¨ï¼Œæ—¥å¿—æ–‡ä»¶å­˜å‚¨åœ¨ PROJECT_ROOT/logs/ ç›®å½•ä¸‹ï¼Œæ—¥å¿—çº§åˆ«ä¸º logfile_levelã€‚
    return _logger


logger = define_log_level()


if __name__ == "__main__":
    logger.info("Starting application")
    logger.debug("Debug message")
    logger.warning("Warning message")
    logger.error("Error message")
    logger.critical("Critical message")

    try:
        raise ValueError("Test error")
    except Exception as e:
        logger.exception(f"An error occurred: {e}")
"""
æ§åˆ¶å°è¾“å‡º
2025-03-10 08:45:07.324 | INFO     | __main__:<module>:39 - Starting application
2025-03-10 08:45:07.324 | WARNING  | __main__:<module>:41 - Warning message
2025-03-10 08:45:07.324 | ERROR    | __main__:<module>:42 - Error message
2025-03-10 08:45:07.324 | CRITICAL | __main__:<module>:43 - Critical message
2025-03-10 08:45:07.324 | ERROR    | __main__:<module>:48 - An error occurred: Test error
Traceback (most recent call last):

> File "/workspace/xsc_workspace/OpenManus/app/logger.py", line 46, in <module>
    raise ValueError("Test error")

ValueError: Test error


è€Œlogæ–‡ä»¶åˆ™å¤šå‡º debugçº§åˆ«çš„æ—¥å¿—
2025-03-10 08:45:07.324 | DEBUG    | __main__:<module>:40 - Debug message



æ§åˆ¶å°æ—¥å¿—çº§åˆ«ï¼šprint_level="INFO",æ–‡ä»¶æ—¥å¿—çº§åˆ«ï¼šlogfile_level="DEBUG"
åªæœ‰çº§åˆ«å¤§äºæˆ–ç­‰äº INFO çš„æ—¥å¿—æ¶ˆæ¯ï¼ˆINFO, WARNING, ERROR, CRITICALï¼‰æ‰ä¼šè¾“å‡ºåˆ°æ§åˆ¶å°ã€‚

logger.debug("This is a debug message")  # çº§åˆ«: DEBUG (10)
logger.info("This is an info message")    # çº§åˆ«: INFO (20)
logger.warning("This is a warning message")  # çº§åˆ«: WARNING (30)
logger.error("This is an error message")  # çº§åˆ«: ERROR (40)
logger.critical("This is a critical message")  # çº§åˆ«: CRITICAL (50)
"""
```
### agentè®¾è®¡
#### agent state
åœ¨schema.pyæ–‡ä»¶ä¸­ç®€å•å°è£…äº†è¡¨ç¤ºagentçŠ¶æ€çš„ç±»
```python
from enum import Enum
class AgentState(str, Enum):
    """
        åŒæ—¶ç»§æ‰¿äº† str å’Œ Enumã€‚è¿™ç§è®¾è®¡çš„ä¸»è¦ç›®çš„æ˜¯è®©æšä¸¾æˆå‘˜æ—¢æ˜¯ä¸€ä¸ªæšä¸¾å€¼ï¼Œåˆæ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ã€‚
        # ç›´æ¥ä½œä¸ºå­—ç¬¦ä¸²ä½¿ç”¨
        print(AgentState.RUNNING)  # è¾“å‡º: RUNNING
        # å¦åˆ™éœ€è¦è°ƒç”¨.value
        print(AgentState.RUNNING.value) 
    """
    """Agent execution states"""

    IDLE = "IDLE"
    RUNNING = "RUNNING"
    FINISHED = "FINISHED"
    ERROR = "ERROR"
```

#### agent memory
åŒæ ·åœ¨schema.pyæ–‡ä»¶ä¸­ï¼Œ
1. è¿™é‡Œé¦–å…ˆåŸºäºfunction callingçš„é€»è¾‘ï¼Œ
å°è£…äº† function -> tool call
```python
class Function(BaseModel):
    name: str
    arguments: str


class ToolCall(BaseModel):
    """Represents a tool/function call in a message"""

    id: str
    type: str = "function"
    function: Function
2. ç„¶åæ˜¯å¯¹ä¸æ¨¡å‹chatæ—¶çš„æ¶ˆæ¯ç±»å‹çš„å°è£…ï¼Œ å¯ä»¥ç›´æ¥ä½¿ç”¨Messageç±»æ–¹ä¾¿çš„åˆ›é€ å¯¹è¯æ¶ˆæ¯
```
```python
class Message(BaseModel):
    """Represents a chat message in the conversation"""

    role: Literal["system", "user", "assistant", "tool"] = Field(...)
    content: Optional[str] = Field(default=None)
    tool_calls: Optional[List[ToolCall]] = Field(default=None)# tool callæ˜¯ä¸€ä¸ªlist
    name: Optional[str] = Field(default=None)
    tool_call_id: Optional[str] = Field(default=None)

    def __add__(self, other) -> List["Message"]:
        """æ”¯æŒ Message + list æˆ– Message + Message çš„æ“ä½œ"""
        if isinstance(other, list):
            return [self] + other
        elif isinstance(other, Message):
            return [self, other]
        else:
            raise TypeError(
                f"unsupported operand type(s) for +: '{type(self).__name__}' and '{type(other).__name__}'"
            )

    def __radd__(self, other) -> List["Message"]:
        """æ”¯æŒ list + Message çš„æ“ä½œ"""
        if isinstance(other, list):
            return other + [self]
        else:
            raise TypeError(
                f"unsupported operand type(s) for +: '{type(other).__name__}' and '{type(self).__name__}'"
            )

    def to_dict(self) -> dict:
        """Convert message to dictionary format"""
        message = {"role": self.role}
        if self.content is not None:
            message["content"] = self.content
        if self.tool_calls is not None:
            message["tool_calls"] = [tool_call.dict() for tool_call in self.tool_calls]
        if self.name is not None:
            message["name"] = self.name
        if self.tool_call_id is not None:
            message["tool_call_id"] = self.tool_call_id
        return message

    @classmethod
    def user_message(cls, content: str) -> "Message":
        """Create a user message"""
        return cls(role="user", content=content)

    @classmethod
    def system_message(cls, content: str) -> "Message":
        """Create a system message"""
        return cls(role="system", content=content)

    @classmethod
    def assistant_message(cls, content: Optional[str] = None) -> "Message":
        """Create an assistant message"""
        return cls(role="assistant", content=content)

    @classmethod
    def tool_message(cls, content: str, name, tool_call_id: str) -> "Message":
        """Create a tool message"""
        return cls(role="tool", content=content, name=name, tool_call_id=tool_call_id)

    @classmethod
    def from_tool_calls(
        cls, tool_calls: List[Any], content: Union[str, List[str]] = "", **kwargs
    ):
        """Create ToolCallsMessage from raw tool calls.

        Args:
            tool_calls: Raw tool calls from LLM
            content: Optional message content
        """
        formatted_calls = [
            {"id": call.id, "function": call.function.model_dump(), "type": "function"}
            for call in tool_calls
        ]
        return cls(
            role="assistant", content=content, tool_calls=formatted_calls, **kwargs
        )
```
3. æœ€åæ˜¯å¯¹agentè®°å¿†èƒ½åŠ›çš„å°è£… 
é»˜è®¤agentæœ€å¤§æœ‰100æ¡æ¶ˆæ¯å®¹é‡
```python
class Memory(BaseModel):
    messages: List[Message] = Field(default_factory=list)
    max_messages: int = Field(default=100)

    def add_message(self, message: Message) -> None:
        """Add a message to memory"""
        self.messages.append(message)
        # Optional: Implement message limit
        if len(self.messages) > self.max_messages:
            self.messages = self.messages[-self.max_messages :]
            # è¶…å‡º100æ¡messageåï¼Œå…ˆè¿›å…ˆå‡ºæ‰ä¹‹å‰çš„è®°å¿†

    def add_messages(self, messages: List[Message]) -> None:
        """Add multiple messages to memory"""
        self.messages.extend(messages)

    def clear(self) -> None:
        """Clear all messages"""
        self.messages.clear()

    # è·å–æœ€è¿‘næ¡è®°å¿†
    def get_recent_messages(self, n: int) -> List[Message]:
        """Get n most recent messages"""
        return self.messages[-n:]

    def to_dict_list(self) -> List[dict]:
        """Convert messages to list of dicts"""
        return [msg.to_dict() for msg in self.messages]
```

#### llm chat
llm.pyä¸­ä½¿ç”¨ LLMç±»å°è£…äº† ä¸å¤§æ¨¡å‹äº¤äº’çš„æ“ä½œ,
askä¸ask_toolï¼Œä¸€ä¸ªå¯¹è¯ï¼Œä¸€ä¸ªfunction calling
æ³¨æ„éƒ½æ˜¯async
```python
from openai import (
    APIError,
    AsyncAzureOpenAI,
    AsyncOpenAI,# ï¼ï¼ï¼
    AuthenticationError,
    OpenAIError,
    RateLimitError,
)
"""
deepseek:
è¿™æ®µä»£ç åœ¨å¤šçº¿ç¨‹ç¯å¢ƒä¸‹æ˜¯ä¸å®‰å…¨çš„ã€‚
å…·ä½“æ¥è¯´ï¼Œ__new__ æ–¹æ³•ä¸­çš„å®ä¾‹åˆ›å»ºå’Œå­˜å‚¨æ“ä½œï¼ˆcls._instances[config_name] = instanceï¼‰å¯èƒ½ä¼šå¼•å‘ç«æ€æ¡ä»¶ï¼ˆRace Conditionï¼‰ï¼Œ
å¯¼è‡´å¤šä¸ªçº¿ç¨‹åŒæ—¶åˆ›å»ºå¤šä¸ªå®ä¾‹ï¼Œä»è€Œç ´åå•ä¾‹æ¨¡å¼çš„è®¾è®¡ã€‚
"""
class LLM:
    # ä¸llmäº¤äº’çš„ç±»
    _instances: Dict[str, "LLM"] = {}

    def __new__(
        cls, config_name: str = "default", llm_config: Optional[LLMSettings] = None
    ):
        if config_name not in cls._instances:
            instance = super().__new__(cls)
            instance.__init__(config_name, llm_config)
            cls._instances[config_name] = instance
        return cls._instances[config_name]

    def __init__(
        self, config_name: str = "default", llm_config: Optional[LLMSettings] = None
    ):
        if not hasattr(self, "client"):  # Only initialize if not already initialized
            llm_config = llm_config or config.llm
            llm_config = llm_config.get(config_name, llm_config["default"])
            self.model = llm_config.model
            self.max_tokens = llm_config.max_tokens
            self.temperature = llm_config.temperature
            self.api_type = llm_config.api_type
            self.api_key = llm_config.api_key
            self.api_version = llm_config.api_version
            self.base_url = llm_config.base_url
            if self.api_type == "azure":
                self.client = AsyncAzureOpenAI(
                    base_url=self.base_url,
                    api_key=self.api_key,
                    api_version=self.api_version,
                )
            else:
                self.client = AsyncOpenAI(api_key=self.api_key, base_url=self.base_url)

    # æ ¼å¼åŒ–å‘æ¨¡å‹å‘é€çš„message
    # messageå¿…é¡»åŒ…æ‹¬contentæˆ–è€…tool_calls
    # ä¸€ä¸ªmessageçš„roleå¿…é¡»åœ¨["system", "user", "assistant", "tool"]
    @staticmethod
    def format_messages(messages: List[Union[dict, Message]]) -> List[dict]:
        """
        Format messages for LLM by converting them to OpenAI message format.

        Args:
            messages: List of messages that can be either dict or Message objects

        Returns:
            List[dict]: List of formatted messages in OpenAI format

        Raises:
            ValueError: If messages are invalid or missing required fields
            TypeError: If unsupported message types are provided

        Examples:
            >>> msgs = [
            ...     Message.system_message("You are a helpful assistant"),
            ...     {"role": "user", "content": "Hello"},
            ...     Message.user_message("How are you?")
            ... ]
            >>> formatted = LLM.format_messages(msgs)
        """
        formatted_messages = []

        for message in messages:
            if isinstance(message, dict):
                # If message is already a dict, ensure it has required fields
                if "role" not in message:
                    raise ValueError("Message dict must contain 'role' field")
                formatted_messages.append(message)
            elif isinstance(message, Message):
                # If message is a Message object, convert it to dict
                formatted_messages.append(message.to_dict())
            else:
                raise TypeError(f"Unsupported message type: {type(message)}")

        # Validate all messages have required fields
        for msg in formatted_messages:
            if msg["role"] not in ["system", "user", "assistant", "tool"]:
                raise ValueError(f"Invalid role: {msg['role']}")
            if "content" not in msg and "tool_calls" not in msg:
                raise ValueError(
                    "Message must contain either 'content' or 'tool_calls'"
                )

        return formatted_messages
    """
    ä½¿ç”¨äº† tenacity åº“æ¥å®ç°é‡è¯•æœºåˆ¶
    from tenacity import retry, stop_after_attempt, wait_random_exponential
    @retry(
        wait=wait_random_exponential(min=1, max=60),
        stop=stop_after_attempt(6),
    )
1.å½“è¢«è£…é¥°çš„å‡½æ•°æŠ›å‡ºå¼‚å¸¸æ—¶ï¼Œ@retry ä¼šæ ¹æ®é…ç½®çš„é‡è¯•ç­–ç•¥å†³å®šæ˜¯å¦é‡æ–°æ‰§è¡Œå‡½æ•°
wait_random_exponential æ˜¯ä¸€ç§éšæœºæŒ‡æ•°é€€é¿ç­–ç•¥ï¼Œå®ƒçš„ç‰¹ç‚¹æ˜¯ï¼š
æŒ‡æ•°å¢é•¿ï¼šæ¯æ¬¡é‡è¯•çš„ç­‰å¾…æ—¶é—´ä¼šé€æ¸å¢åŠ ï¼Œé¿å…é¢‘ç¹é‡è¯•å¯¹ç³»ç»Ÿé€ æˆå‹åŠ›ã€‚
éšæœºæ€§ï¼šåœ¨æŒ‡æ•°å¢é•¿çš„åŸºç¡€ä¸Šï¼ŒåŠ å…¥éšæœºæ€§ï¼Œé¿å…å¤šä¸ªå®¢æˆ·ç«¯åŒæ—¶é‡è¯•å¯¼è‡´çš„é‡è¯•é£æš´ã€‚
min=1ï¼šæœ€å°ç­‰å¾…æ—¶é—´ä¸º 1 ç§’ã€‚
max=60ï¼šæœ€å¤§ç­‰å¾…æ—¶é—´ä¸º 60 ç§’ã€‚
ä¾‹å¦‚ï¼š
ç¬¬ä¸€æ¬¡é‡è¯•ç­‰å¾…æ—¶é—´å¯èƒ½æ˜¯ 1 åˆ° 2 ç§’ä¹‹é—´çš„éšæœºå€¼ã€‚
ç¬¬äºŒæ¬¡é‡è¯•ç­‰å¾…æ—¶é—´å¯èƒ½æ˜¯ 2 åˆ° 4 ç§’ä¹‹é—´çš„éšæœºå€¼ã€‚
ä»¥æ­¤ç±»æ¨ï¼Œç›´åˆ°è¾¾åˆ°æœ€å¤§å€¼ 60 ç§’ã€‚

2.stop=stop_after_attempt(6)
stop å‚æ•°ç”¨äºæŒ‡å®šåœæ­¢é‡è¯•çš„æ¡ä»¶ã€‚
stop_after_attempt(6) è¡¨ç¤ºæœ€å¤šé‡è¯• 6 æ¬¡ï¼ˆåŒ…æ‹¬æœ€åˆçš„ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼‰ã€‚
å¦‚æœå‡½æ•°åœ¨ 6 æ¬¡å°è¯•åä»ç„¶å¤±è´¥ï¼Œåˆ™åœæ­¢é‡è¯•å¹¶æŠ›å‡ºæœ€åä¸€æ¬¡çš„å¼‚å¸¸ã€‚
    """
    @retry(
        wait=wait_random_exponential(min=1, max=60),
        stop=stop_after_attempt(6),
    )
    async def ask(
        self,
        messages: List[Union[dict, Message]],
        system_msgs: Optional[List[Union[dict, Message]]] = None,
        stream: bool = True,
        temperature: Optional[float] = None,
    ) -> str:
        """
        Send a prompt to the LLM and get the response.

        Args:
            messages: List of conversation messages
            system_msgs: Optional system messages to prepend
            stream (bool): Whether to stream the response
            temperature (float): Sampling temperature for the response

        Returns:
            str: The generated response

        Raises:
            ValueError: If messages are invalid or response is empty
            OpenAIError: If API call fails after retries
            Exception: For unexpected errors
        """
        try:
            # Format system and user messages
            if system_msgs:
                system_msgs = self.format_messages(system_msgs)
                messages = system_msgs + self.format_messages(messages)
            else:
                messages = self.format_messages(messages)

            if not stream:
                # Non-streaming request
                response = await self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    max_tokens=self.max_tokens,
                    temperature=temperature or self.temperature,
                    stream=False,
                )
                if not response.choices or not response.choices[0].message.content:
                    raise ValueError("Empty or invalid response from LLM")
                return response.choices[0].message.content

            # Streaming request
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=self.max_tokens,
                temperature=temperature or self.temperature,
                stream=True,
            )

            collected_messages = []
            async for chunk in response:
                chunk_message = chunk.choices[0].delta.content or ""
                collected_messages.append(chunk_message)
                print(chunk_message, end="", flush=True)

            print()  # Newline after streaming
            full_response = "".join(collected_messages).strip()
            if not full_response:
                raise ValueError("Empty response from streaming LLM")
            return full_response

        except ValueError as ve:
            logger.error(f"Validation error: {ve}")
            raise
        except OpenAIError as oe:
            logger.error(f"OpenAI API error: {oe}")
            raise
        except Exception as e:
            logger.error(f"Unexpected error in ask: {e}")
            raise

    @retry(
        wait=wait_random_exponential(min=1, max=60),
        stop=stop_after_attempt(6),
    )
    async def ask_tool(
        self,
        messages: List[Union[dict, Message]],
        system_msgs: Optional[List[Union[dict, Message]]] = None,
        timeout: int = 60,
        tools: Optional[List[dict]] = None,
        tool_choice: Literal["none", "auto", "required"] = "auto",
        temperature: Optional[float] = None,
        **kwargs,
    ):
        """
        Ask LLM using functions/tools and return the response.

        Args:
            messages: List of conversation messages
            system_msgs: Optional system messages to prepend
            timeout: Request timeout in seconds
            tools: List of tools to use
            tool_choice: Tool choice strategy
            temperature: Sampling temperature for the response
            **kwargs: Additional completion arguments

        Returns:
            ChatCompletionMessage: The model's response

        Raises:
            ValueError: If tools, tool_choice, or messages are invalid
            OpenAIError: If API call fails after retries
            Exception: For unexpected errors
        """
        try:
            # Validate tool_choice
            if tool_choice not in ["none", "auto", "required"]:
                raise ValueError(f"Invalid tool_choice: {tool_choice}")

            # Format messages
            if system_msgs:
                system_msgs = self.format_messages(system_msgs)
                messages = system_msgs + self.format_messages(messages)
            else:
                messages = self.format_messages(messages)

            # Validate tools if provided
            if tools:
                for tool in tools:
                    if not isinstance(tool, dict) or "type" not in tool:
                        raise ValueError("Each tool must be a dict with 'type' field")

            # Set up the completion request
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature or self.temperature,
                max_tokens=self.max_tokens,
                tools=tools,
                tool_choice=tool_choice,
                timeout=timeout,
                **kwargs,
            )
            # tool callåº”è¯¥æ˜¯ä¸æ”¯æŒ stream
            
            # Check if response is valid
            if not response.choices or not response.choices[0].message:
                print(response)
                raise ValueError("Invalid or empty response from LLM")

            return response.choices[0].message

        except ValueError as ve:
            logger.error(f"Validation error in ask_tool: {ve}")
            raise
        except OpenAIError as oe:
            if isinstance(oe, AuthenticationError):
                logger.error("Authentication failed. Check API key.")
            elif isinstance(oe, RateLimitError):
                logger.error("Rate limit exceeded. Consider increasing retry attempts.")
            elif isinstance(oe, APIError):
                logger.error(f"API error: {oe}")
            raise
        except Exception as e:
            logger.error(f"Unexpected error in ask_tool: {e}")
            raise
```

#### tool é…ç½®
é¦–å…ˆæ˜¯å¼‚å¸¸é…ç½®ï¼Œåœ¨exceptions.pyé‡Œå®šä¹‰äº†toolä½¿ç”¨å‡ºç°error
```python
class ToolError(Exception):
    """Raised when a tool encounters an error."""

    def __init__(self, message):
        self.message = message
```
##### å·¥å…·æ€»è§ˆ
ç„¶åapp/tool/æ–‡ä»¶å¤¹ä¸‹å°è£…äº†å„ç§ä½¿ç”¨å·¥å…·ï¼Œ

| BashTool |æ‰§è¡Œbashå‘½ä»¤|
| :- | :-: |
| PythonExecuteTool | æ‰§è¡Œpythonå‘½ä»¤ |
| FileSaverTool | æ–‡ä»¶ä¿å­˜ | 


![alt text](assets/openmanus/image-2.png)

##### ï¼ï¼TerminateTool ï¼ï¼

å¯¹äºTerminateå·¥å…· å°†è¿”å› sucess failureçš„çŠ¶æ€ï¼Œç”±æ­¤å†³å®šæ˜¯å¦ä»»åŠ¡å®Œæˆ
*ç»ˆç»“é€»è¾‘å³ï¼š å·¥å…·æ‰§è¡Œè¿”å›resultåï¼Œæ¯æ¬¡æŸ¥çœ‹å·¥å…·åç§°ï¼Œè‹¥ä¸ºspecial toolå³Terminateï¼Œåˆ™è¿”å›trueå¹¶ç»ˆç»“*

```python
_TERMINATE_DESCRIPTION = """Terminate the interaction when the request is met OR if the assistant cannot proceed further with the task."""


class Terminate(BaseTool):
    name: str = "terminate"
    description: str = _TERMINATE_DESCRIPTION
    parameters: dict = {
        "type": "object",
        "properties": {
            "status": {
                "type": "string",
                "description": "The finish status of the interaction.",
                "enum": ["success", "failure"],
            }
        },
        "required": ["status"],
    }

    async def execute(self, status: str) -> str:
        """Finish the current execution"""
        return f"The interaction has been completed with status: {status}"
```


#### å…·ä½“agentä¸promptè®¾è®¡
[çŸ¥ä¹ä¸Šå¤§ä½¬ çš„ åˆ†æå›¾å¦‚ä¸‹](https://www.zhihu.com/question/14322364598)
![alt text](assets/openmanus/image-1.png)

ç®€å•æ¦‚è§ˆï¼Œ
1. åœ¨BaseAentåŸºç±»ä¸­ï¼Œè®¾è®¡runä¸ºagentæ‰§è¡Œæ–¹æ³•ï¼Œå…·ä½“å³ä¸ºè°ƒç”¨stepå‡½æ•°(ç›´è‡³å¾ªç¯æŒ‡å®šmax_stepsæ­¥éª¤ æˆ– ä»»åŠ¡å®Œæˆ)ï¼ŒåŒæ—¶å¤„ç†å¼‚å¸¸çŠ¶æ€å¦‚agenté˜»å¡ï¼Œ
**è€Œåœ¨è¿™é‡ŒæŠŠstepå‡½æ•°ä½œä¸ºæŠ½è±¡å‡½æ•°**

2. ReActAgentç»§æ‰¿BaseAgent, å®ç°stepå‡½æ•°ï¼Œå…·ä½“æµç¨‹ä¸ºæ‰§è¡Œä¸€æ¬¡thinkå’Œactå‡½æ•°
**å¹¶ä¸”æŠŠthinkå’Œactè®¾è®¡ä¸ºæŠ½è±¡å‡½æ•°**

3. ToolCallAgentç»§æ‰¿ReActAgentï¼Œåˆæ­¥å®ç°thinkå’Œactå‡½æ•°ï¼Œ
thinké‡Œå°±è°ƒç”¨llm.ask_toolå‡½æ•°ä¸æ¨¡å‹äº¤äº’ï¼Œå¹¶æ ¹æ®æ¨¡å‹è¿”å›çš„tool_callså†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·
actå‡½æ•°é‡Œè°ƒç”¨execute_toolæ‰§è¡Œå·¥å…·è°ƒç”¨å¹¶å°†è¿”å›ç»“æœå°è£…Message

4. planningAgentã€SWEAgentã€Manusç»§æ‰¿ToolCallAgentå¹¶å„è‡ªè¿›ä¸€æ­¥å®ç°ä¸åŒåŠŸèƒ½


##### BaseAgent
```python
class BaseAgent(BaseModel, ABC):
    """Abstract base class for managing agent state and execution.

    Provides foundational functionality for state transitions, memory management,
    and a step-based execution loop. Subclasses must implement the `step` method.
    """

    # Core attributes
    name: str = Field(..., description="Unique name of the agent")
    description: Optional[str] = Field(None, description="Optional agent description")

    # Prompts
    system_prompt: Optional[str] = Field(
        None, description="System-level instruction prompt"
    )
    next_step_prompt: Optional[str] = Field(
        None, description="Prompt for determining next action"
    )

    # Dependencies
    llm: LLM = Field(default_factory=LLM, description="Language model instance")
    memory: Memory = Field(default_factory=Memory, description="Agent's memory store")
    state: AgentState = Field(
        default=AgentState.IDLE, description="Current agent state"
    )

    # Execution control
    max_steps: int = Field(default=10, description="Maximum steps before termination")
    current_step: int = Field(default=0, description="Current step in execution")

    duplicate_threshold: int = 2

    class Config:
        #Config ç±»æ˜¯ä¸€ä¸ªç”¨äºé…ç½® Pydantic æ¨¡å‹è¡Œä¸ºçš„å†…éƒ¨ç±»ã€‚
        
        # è¿™ä¸ªé…ç½®å…è®¸åœ¨ Pydantic æ¨¡å‹ä¸­ä½¿ç”¨ä»»æ„ç±»å‹çš„å­—æ®µï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼ŒPydantic ä¼šæ£€æŸ¥å­—æ®µçš„ç±»å‹æ˜¯å¦ç¬¦åˆæ³¨è§£ã€‚
        #å¦‚æœå­—æ®µçš„ç±»å‹ä¸åœ¨ Pydantic çš„å†…ç½®æ”¯æŒèŒƒå›´å†…ï¼Œä¼šæŠ›å‡ºé”™è¯¯ã€‚
        arbitrary_types_allowed = True
        extra = "allow"  # Allow extra fields for flexibility in subclasses

    @model_validator(mode="after")
    def initialize_agent(self) -> "BaseAgent":
        # ä½¿ç”¨ @model_validator(mode="after") è£…é¥°ã€‚ä¼šåœ¨pydanticæ¨¡å‹çš„æ‰€æœ‰å­—æ®µè¢«éªŒè¯ä¹‹åæ‰§è¡Œã€‚
        """Initialize agent with default settings if not provided."""
        if self.llm is None or not isinstance(self.llm, LLM):
            self.llm = LLM(config_name=self.name.lower())
        if not isinstance(self.memory, Memory):
            self.memory = Memory()
        return self

    @asynccontextmanager
    # ç”¨äºå°†ä¸€ä¸ªå¼‚æ­¥ç”Ÿæˆå™¨å‡½æ•°è½¬æ¢ä¸ºå¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨ã€‚
    # å®ƒç®€åŒ–äº†å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨çš„å®šä¹‰ï¼Œé¿å…äº†æ‰‹åŠ¨å®ç° __aenter__ å’Œ __aexit__ æ–¹æ³•ã€‚
"""
import asyncio
from contextlib import asynccontextmanager

# å®šä¹‰ä¸€ä¸ªå¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨
@asynccontextmanager
async def async_resource_manager():
    print("Acquiring resource...")
    resource = "Some Resource"
    try:
        yield resource  # å°†èµ„æºæä¾›ç»™ async with å—
    finally:
        print("Releasing resource...")

# ä½¿ç”¨å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨
async def main():
    async with async_resource_manager() as resource:
        print(f"Using resource: {resource}")
        await asyncio.sleep(1)  # æ¨¡æ‹Ÿå¼‚æ­¥æ“ä½œ
        print(f"operate successfully!")

# è¿è¡Œå¼‚æ­¥å‡½æ•°
asyncio.run(main())

ç»“æœï¼š
Acquiring resource...
Using resource: Some Resource
operate successfully!
Releasing resource...

"""
    #è¿™ä¸ªå¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨çš„ yieldä¹‹åçš„å¤„ç†å¾ˆå¥‡æ€ªï¼Ÿ
    async def state_context(self, new_state: AgentState):
        """Context manager for safe agent state transitions.

        Args:
            new_state: The state to transition to during the context.

        Yields:
            None: Allows execution within the new state.

        Raises:
            ValueError: If the new_state is invalid.
        """
        if not isinstance(new_state, AgentState):
            raise ValueError(f"Invalid state: {new_state}")

        previous_state = self.state
        self.state = new_state
        try:
            yield
        except Exception as e:
            self.state = AgentState.ERROR  # Transition to ERROR on failure
            raise e
        finally:
            self.state = previous_state  # Revert to previous state

    # å°†messageæ·»åŠ åˆ°agentçš„memoryä¸­
    def update_memory(
        self,
        role: Literal["user", "system", "assistant", "tool"],
        content: str,
        **kwargs,
    ) -> None:
        """Add a message to the agent's memory.

        Args:
            role: The role of the message sender (user, system, assistant, tool).
            content: The message content.
            **kwargs: Additional arguments (e.g., tool_call_id for tool messages).

        Raises:
            ValueError: If the role is unsupported.
        """
        message_map = {
            "user": Message.user_message,
            "system": Message.system_message,
            "assistant": Message.assistant_message,
            "tool": lambda content, **kw: Message.tool_message(content, **kw),
        }

        if role not in message_map:
            raise ValueError(f"Unsupported message role: {role}")

        msg_factory = message_map[role]
        msg = msg_factory(content, **kwargs) if role == "tool" else msg_factory(content)
        self.memory.add_message(msg)

    #ä¸»è¦è¿è¡Œå‡½æ•°
    # æ³¨æ„stucké˜»å¡æ—¶çš„å¤„ç†æ“ä½œ
    async def run(self, request: Optional[str] = None) -> str:
        """Execute the agent's main loop asynchronously.

        Args:
            request: Optional initial user request to process.

        Returns:
            A string summarizing the execution results.

        Raises:
            RuntimeError: If the agent is not in IDLE state at start.
        """
        if self.state != AgentState.IDLE:
            raise RuntimeError(f"Cannot run agent from state: {self.state}")

        if request:
            self.update_memory("user", request)

        results: List[str] = []
        async with self.state_context(AgentState.RUNNING):
            while (
                self.current_step < self.max_steps and self.state != AgentState.FINISHED
            ):
                self.current_step += 1
                logger.info(f"Executing step {self.current_step}/{self.max_steps}")
                step_result = await self.step()

                # Check for stuck state
                if self.is_stuck():
                    self.handle_stuck_state()

                results.append(f"Step {self.current_step}: {step_result}")

            if self.current_step >= self.max_steps:
                results.append(f"Terminated: Reached max steps ({self.max_steps})")

        return "\n".join(results) if results else "No steps executed"

    @abstractmethod
    async def step(self) -> str:
        """Execute a single step in the agent's workflow.

        Must be implemented by subclasses to define specific behavior.
        """

    def handle_stuck_state(self):
        """Handle stuck state by adding a prompt to change strategy"""
        stuck_prompt = "\
        Observed duplicate responses. Consider new strategies and avoid repeating ineffective paths already attempted."
        self.next_step_prompt = f"{stuck_prompt}\n{self.next_step_prompt}"
        logger.warning(f"Agent detected stuck state. Added prompt: {stuck_prompt}")

    def is_stuck(self) -> bool:
        """Check if the agent is stuck in a loop by detecting duplicate content"""
        if len(self.memory.messages) < 2:
            return False

        last_message = self.memory.messages[-1]
        if not last_message.content:
            return False

        # Count identical content occurrences
        duplicate_count = sum(
            1
            for msg in reversed(self.memory.messages[:-1])
            if msg.role == "assistant" and msg.content == last_message.content
        )

        return duplicate_count >= self.duplicate_threshold

    @property
    # é€šè¿‡ @propertyï¼Œä½ å¯ä»¥å°†ä¸€ä¸ªæ–¹æ³•è½¬æ¢ä¸ºåªè¯»å±æ€§
    def messages(self) -> List[Message]:
        """Retrieve a list of messages from the agent's memory."""
        return self.memory.messages

    @messages.setter
    def messages(self, value: List[Message]):
        """Set the list of messages in the agent's memory."""
        self.memory.messages = value
```

##### ReActAgent
```python
class ReActAgent(BaseAgent, ABC):
    name: str
    description: Optional[str] = None

    system_prompt: Optional[str] = None
    next_step_prompt: Optional[str] = None

    llm: Optional[LLM] = Field(default_factory=LLM)
    memory: Memory = Field(default_factory=Memory)
    state: AgentState = AgentState.IDLE

    max_steps: int = 10
    current_step: int = 0

    @abstractmethod
    async def think(self) -> bool:
        """Process current state and decide next action"""

    @abstractmethod
    async def act(self) -> str:
        """Execute decided actions"""

    async def step(self) -> str:
        """Execute a single step: think and act."""
        should_act = await self.think()
        if not should_act:#å¦‚ä¸éœ€è¦toolï¼Œç›´æ¥è¿”å›
            return "Thinking complete - no action needed"
        return await self.act()
```

#### ToolAgent
prompt/toolcall.pyä¸­çš„ promptå®šä¹‰

```python
SYSTEM_PROMPT = "You are an agent that can execute tool calls"

NEXT_STEP_PROMPT = (
    "If you want to stop interaction, use `terminate` tool/function call."
)

```
ä¸‹è¿°ä¸ºTool Agentï¼Œæ³¨æ„ä¸‹finishé€»è¾‘

å¯¹äºTerminateå·¥å…· å°†è¿”å› sucess failureçš„çŠ¶æ€ï¼Œç”±æ­¤å†³å®šæ˜¯å¦ä»»åŠ¡å®Œæˆ
*ç»ˆç»“é€»è¾‘å³ï¼š å·¥å…·æ‰§è¡Œè¿”å›resultåï¼Œæ¯æ¬¡æŸ¥çœ‹å·¥å…·åç§°ï¼Œè‹¥ä¸ºspecial toolå³Terminateï¼Œåˆ™è¿”å›trueå¹¶ç»ˆç»“*

```python
TOOL_CALL_REQUIRED = "Tool calls required but none provided"


class ToolCallAgent(ReActAgent):
    """Base agent class for handling tool/function calls with enhanced abstraction"""


    """
å…¬æœ‰å±æ€§ï¼š
çˆ¶ç±»ä¸­å®šä¹‰çš„å…¬æœ‰å±æ€§ï¼ˆå¦‚ name, description, system_prompt ç­‰ï¼‰å¯ä»¥ç›´æ¥è¢«å­ç±»è®¿é—®ã€‚
å­ç±»å¯ä»¥è¦†ç›–çˆ¶ç±»çš„å±æ€§ï¼Œä¹Ÿå¯ä»¥ç›´æ¥ä½¿ç”¨çˆ¶ç±»çš„å±æ€§ã€‚

ç§æœ‰å±æ€§ï¼š
çˆ¶ç±»ä¸­ä»¥å•ä¸‹åˆ’çº¿ _ æˆ–åŒä¸‹åˆ’çº¿ __ å¼€å¤´çš„å±æ€§æ˜¯ç§æœ‰çš„ï¼Œå­ç±»ä¸èƒ½ç›´æ¥è®¿é—®ã€‚
å•ä¸‹åˆ’çº¿ _ å¼€å¤´çš„å±æ€§æ˜¯çº¦å®šä¸Šçš„ç§æœ‰å±æ€§ï¼Œå­ç±»å¯ä»¥è®¿é—®ä½†ä¸å»ºè®®ã€‚
åŒä¸‹åˆ’çº¿ __ å¼€å¤´çš„å±æ€§ä¼šè¢« Python è¿›è¡Œåç§°ä¿®é¥°ï¼ˆname manglingï¼‰ï¼Œå­ç±»æ— æ³•ç›´æ¥è®¿é—®ã€‚

Pydantic çš„ç‰¹æ®Šè¡Œä¸ºï¼š
Pydantic æ¨¡å‹ä¸­çš„å­—æ®µï¼ˆé€šè¿‡ Field å®šä¹‰çš„å±æ€§ï¼‰ä¼šè¢« Pydantic è‡ªåŠ¨å¤„ç†ï¼Œå­ç±»å¯ä»¥ç»§æ‰¿å’Œè¦†ç›–è¿™äº›å­—æ®µã€‚
Pydantic ä¼šåˆå¹¶çˆ¶ç±»å’Œå­ç±»çš„å­—æ®µå®šä¹‰ï¼Œç”Ÿæˆæœ€ç»ˆçš„æ¨¡å‹ç»“æ„ã€‚
    """

    name: str = "toolcall"
    description: str = "an agent that can execute tool calls."

    system_prompt: str = SYSTEM_PROMPT
    next_step_prompt: str = NEXT_STEP_PROMPT

    available_tools: ToolCollection = ToolCollection(
        CreateChatCompletion(), Terminate()
    )
    tool_choices: Literal["none", "auto", "required"] = "auto"

    #Terminate toolä½œä¸º special toolï¼Œ å†³å®šäº†æ˜¯å¦è½¬å˜agentä¸º finishçŠ¶æ€
    special_tool_names: List[str] = Field(default_factory=lambda: [Terminate().name])

    tool_calls: List[ToolCall] = Field(default_factory=list)

    max_steps: int = 30

    async def think(self) -> bool:
        """Process current state and decide next actions using tools"""
        if self.next_step_prompt:
            user_msg = Message.user_message(self.next_step_prompt)
            self.messages += [user_msg]

        # Get response with tool options
        response = await self.llm.ask_tool(
            messages=self.messages,
            system_msgs=[Message.system_message(self.system_prompt)]
            if self.system_prompt
            else None,
            tools=self.available_tools.to_params(),
            tool_choice=self.tool_choices,
        )
        self.tool_calls = response.tool_calls

        # Log response info
        logger.info(f"âœ¨ {self.name}'s thoughts: {response.content}")
        logger.info(
            f"ğŸ› ï¸ {self.name} selected {len(response.tool_calls) if response.tool_calls else 0} tools to use"
        )
        if response.tool_calls:
            logger.info(
                f"ğŸ§° Tools being prepared: {[call.function.name for call in response.tool_calls]}"
            )

        try:
            # Handle different tool_choices modes
            if self.tool_choices == "none":
                if response.tool_calls:
                    logger.warning(
                        f"ğŸ¤” Hmm, {self.name} tried to use tools when they weren't available!"
                    )
                if response.content:
                    self.memory.add_message(Message.assistant_message(response.content))
                    return True
                return False

            # Create and add assistant message
            assistant_msg = (
                Message.from_tool_calls(
                    content=response.content, tool_calls=self.tool_calls
                )
                if self.tool_calls
                else Message.assistant_message(response.content)
            )
            self.memory.add_message(assistant_msg)

            if self.tool_choices == "required" and not self.tool_calls:
                return True  # Will be handled in act()

            # For 'auto' mode, continue with content if no commands but content exists
            if self.tool_choices == "auto" and not self.tool_calls:
                return bool(response.content)

            return bool(self.tool_calls)
        except Exception as e:
            logger.error(f"ğŸš¨ Oops! The {self.name}'s thinking process hit a snag: {e}")
            self.memory.add_message(
                Message.assistant_message(
                    f"Error encountered while processing: {str(e)}"
                )
            )
            return False

    async def act(self) -> str:
        """Execute tool calls and handle their results"""
        if not self.tool_calls:
            if self.tool_choices == "required":
                raise ValueError(TOOL_CALL_REQUIRED)

            # Return last message content if no tool calls
            return self.messages[-1].content or "No content or commands to execute"

        results = []
        for command in self.tool_calls:
            result = await self.execute_tool(command)
            logger.info(
                f"ğŸ¯ Tool '{command.function.name}' completed its mission! Result: {result}"
            )

            # Add tool response to memory
            tool_msg = Message.tool_message(
                content=result, tool_call_id=command.id, name=command.function.name
            )
            self.memory.add_message(tool_msg)
            results.append(result)

        return "\n\n".join(results)

    async def execute_tool(self, command: ToolCall) -> str:
        """Execute a single tool call with robust error handling"""
        if not command or not command.function or not command.function.name:
            return "Error: Invalid command format"

        name = command.function.name
        if name not in self.available_tools.tool_map:
            return f"Error: Unknown tool '{name}'"

        try:
            # Parse arguments
            args = json.loads(command.function.arguments or "{}")

            # Execute the tool
            logger.info(f"ğŸ”§ Activating tool: '{name}'...")

            # è°ƒç”¨å„ä¸ªå·¥å…·çš„executeæ–¹æ³•å–æ‰§è¡Œ
            result = await self.available_tools.execute(name=name, tool_input=args)

            # Format result for display
            observation = (
                f"Observed output of cmd `{name}` executed:\n{str(result)}"
                if result
                else f"Cmd `{name}` completed with no output"
            )

            # å¯¹äºTerminateå·¥å…· å°†è¿”å› sucess failureçš„çŠ¶æ€ï¼Œç”±æ­¤å†³å®šæ˜¯å¦ä»»åŠ¡å®Œæˆ
            # ç»ˆç»“é€»è¾‘å³ï¼š å·¥å…·æ‰§è¡Œè¿”å›resultåï¼Œæ¯æ¬¡æŸ¥çœ‹å·¥å…·åç§°ï¼Œè‹¥ä¸ºspecial toolå³Terminateï¼Œåˆ™è¿”å›trueå¹¶ç»ˆç»“
            # Handle special tools like `finish`
            await self._handle_special_tool(name=name, result=result)

            return observation
        except json.JSONDecodeError:
            error_msg = f"Error parsing arguments for {name}: Invalid JSON format"
            logger.error(
                f"ğŸ“ Oops! The arguments for '{name}' don't make sense - invalid JSON, arguments:{command.function.arguments}"
            )
            return f"Error: {error_msg}"
        except Exception as e:
            error_msg = f"âš ï¸ Tool '{name}' encountered a problem: {str(e)}"
            logger.error(error_msg)
            return f"Error: {error_msg}"

    async def _handle_special_tool(self, name: str, result: Any, **kwargs):
        """Handle special tool execution and state changes"""
        if not self._is_special_tool(name):
            return

        if self._should_finish_execution(name=name, result=result, **kwargs):
            # Set agent state to finished
            logger.info(f"ğŸ Special tool '{name}' has completed the task!")
            self.state = AgentState.FINISHED

    @staticmethod
    def _should_finish_execution(**kwargs) -> bool:
        """Determine if tool execution should finish the agent"""
        return True

    def _is_special_tool(self, name: str) -> bool:
        """Check if tool name is in special tools list"""
        return name.lower() in [n.lower() for n in self.special_tool_names]
```


#### planning agent






