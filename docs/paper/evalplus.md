
# evalplus
[evalplus github](https://github.com/evalplus/evalplus) 

EvalPlus æ˜¯ LLM4codeçš„ä¸¥è°¨çš„è¯„ä¼°æ¡†æ¶
æ”¯æŒçš„æ•°æ®é›†æœ‰
1) humaneval+ï¼šæ¯”èµ·åŸå§‹humanevalå¤šäº†80å€æµ‹è¯•ç”¨ä¾‹
2) mbpp+ï¼šæ¯”åŸå§‹mbppå¤šäº†35å€æµ‹è¯•ç”¨ä¾‹
3) evalperfï¼šè¯„ä¼°ç”Ÿæˆä»£ç çš„efficiency  é«˜æ•ˆæ€§ã€‚

## ä½¿ç”¨evalplusè¯„æµ‹humanevalå’Œmbpp
```bash
pip install --upgrade "evalplus[vllm] @ git+https://github.com/evalplus/evalplus"
# Or `pip install "evalplus[vllm]" --upgrade` for the latest stable release

evalplus.evaluate --model "ise-uiuc/Magicoder-S-DS-6.7B" \
                  --dataset [humaneval|mbpp]             \
                  --backend vllm                         \
                  --greedy
```

æ³¨æ„ï¼šç„¶åè¿™é‡Œæ˜¯å®šåˆ¶åŒ–ä»£ç ç”Ÿæˆä¸è¯„æµ‹

### ä»£ç ç”Ÿæˆ 
>é¦–å…ˆè¯·æ³¨æ„problemçš„æ ¼å¼ä¸ºï¼š
> task_id
> entry_point   -> å‡½æ•°å
> **prompt  -> å¸¦ç€docstringçš„å‡½æ•°ç­¾å**
> canonical_solution  -> æ ‡å‡†ç­”æ¡ˆï¼ˆé‡æ–°å†™çš„ã€ä¿®å¤äº†HumanEvalçš„bugï¼‰
> base_input -> åŸå§‹humanevalçš„æµ‹è¯•ç”¨ä¾‹è¾“å…¥
> plus_input -> evalplusçš„å¢å¼ºè¾“å…¥

```python
from evalplus.data import get_[human_eval|mbpp]_plus, write_jsonl

def GEN_SIKUTION(prompt):
    ...

samples = [
    dict(task_id=task_id, solution=GEN_SOLUTION(problem["prompt"]))
    for task_id, problem in get_[human_eval|mbpp]_plus().items()
]

write_jsonl("samples.jsonl", samples)
```

>ç„¶åè¯·æ³¨æ„å¾…æ£€æµ‹code generationæ–‡ä»¶çš„æ ¼å¼
>task_id -> ä¿æŒä¸åŠ¨
>solutionä¸completionäºŒé€‰ä¸€
>> solutionåŒ…æ‹¬promptï¼Œå¦‚{"task_id": "HumanEval/?", "solution": "def f():\n    return 1"}
>> completionä¸åŒ…æ‹¬ï¼Œå¦‚{"task_id": "HumanEval/?", "completion": "    return 1"}

### ä»£ç æ¸…æ´—

llmç”Ÿæˆçš„codeå¯èƒ½åŒ…æ‹¬è‡ªç„¶è¯­è¨€æˆ–è€…é¢å¤–çš„ä¸éœ€è¦çš„ä»£ç ï¼Œè¯¥å·¥å…·ä¸ºæ­¤è€Œæ¥

```bash
# ğŸ’¡ If you are storing codes in jsonl:
evalplus.sanitize --samples samples.jsonl
# Sanitized code will be produced to `samples-sanitized.jsonl`

```

ç„¶åå¯ä»¥ä½¿ç”¨ evalplus.syncheck æ£€æµ‹æ¸…æ´—æœ‰æ•ˆæ€§ï¼Œè¯¥å‘½ä»¤å°†æ‰“å°å‡ºä»£ç ç‰‡æ®µå¹¶æŠ¥å‘Šä¸ºä»€ä¹ˆä»–ä»¬é”™è¯¯
```bash
# ğŸ’¡ If you are storing codes in jsonl:
evalplus.syncheck --samples samples.jsonl --dataset [humaneval|mbpp]
```

### ä»£ç è¯„æµ‹

é¦–å…ˆæ˜¯ç›´æ¥æ‰§è¡Œï¼Œä½†æ˜¯æ‰§è¡Œåå‘ç°åå°ä¼šå­˜åœ¨æ— é™å¾ªç¯æ­»è¿›ç¨‹
```bash
evalplus.evaluate --dataset [humaneval|mbpp] --samples samples.jsonl
```

ç„¶åæ˜¯å®‰å…¨çš„dockeræ‰§è¡Œ
```bash
docker run --rm --pull=always -v $(pwd)/evalplus_results:/app ganler/evalplus:latest \
           evalplus.evaluate --dataset humaneval                                     \
           --samples /app/humaneval/ise-uiuc--Magicoder-S-DS-6.7B_vllm_temp_0.0.jsonl
```

ä¸Šè¿°è¯„ä¼°è¾“å‡ºåº”å¦‚ä¸‹ï¼š
```bash
Computing expected output...
Expected outputs computed in 15.18s
Reading samples...
164it [00:04, 37.79it/s]
Evaluating samples...
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:03<00:00, 44.75it/s]
Base
{'pass@1': 0.8841463414634146}
Base + Extra
{'pass@1': 0.768}
```
***Base*** is the pass@k for the ***original HumanEval***

***Base + Extra*** is the pass@k for the our ***HumanEval+ (with extra tests)***

***The "k" includes [1, 10, 100] where k values <= the sample size will be used***

**A cache file** named like samples_eval_results.jsonl will be cached. **Remove it to re-run the evaluation**


#### ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†
```bash
HUMANEVAL_OVERRIDE_PATH="/path/to/HumanEvalPlus.jsonl.gz" evalplus.evaluate --dataset humaneval --samples samples.jsonl
```

### code

