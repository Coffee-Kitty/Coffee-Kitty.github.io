## prompt learning

构造prompt，增强模型 few-shot的能力



![image-20241223202028698](../picture.asset/image-20241223202028698.png)





在pre-training和 fine-tunning间，似乎存在着gap，如下图所示，

<img src="../picture.asset/image-20241223203244053.png" alt="image-20241223203244053" style="zoom:50%;" />

然而，我们可以通过构造prompt模板，缩短这个gap，如下图

<img src="../picture.asset/image-20241223203205751.png" alt="image-20241223203205751" style="zoom:50%;" />

同时，在输出时也可以只采样指定的词汇。

下图为prompt-learning的基本范式：

![image-20241223203534038](../picture.asset/image-20241223203534038.png)



> example:
> <img src="../picture.asset/image-20241223203617370.png" alt="image-20241223203617370" style="zoom: 50%;" />



 ### PTM 选取



<img src="../picture.asset/image-20241223204219460.png" alt="image-20241223204219460" style="zoom: 33%;"  align="left"/>                                                               Auto-regressive这种模式 非常适合大语言模型



擅长生成任务















<img src="../picture.asset/image-20241223204555689.png" alt="image-20241223204555689" style="zoom: 33%;" align="left" /> 

简单的分类任务或者理解任务，RoBERTa可能更合适一些

















### template构造

人为构造，基于任务的特性











## Delta Tuning

高效微调







## RAG

![image-20241224154118521](../picture.asset/image-20241224154118521.png)



![image-20241224154149234](../picture.asset/image-20241224154149234.png)







