{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from transformers import PretrainedConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LMConfig(PretrainedConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        \n",
    "        n_layers: int =8, # 多少层\n",
    "\n",
    "        max_seq_len: int = 512,#最大seq长度\n",
    "        dim:int = 512, #embedding 维度  atten输入总维度\n",
    "\n",
    "\n",
    "        n_heads: int  = 16, # 16个头 #每个head dim为 dim//n_heads\n",
    "        n_kv_heads: int = 8, # 8个 kv头 ，（这里也就是说有8个组，每两个Query head共享一个kv）\n",
    "        \n",
    "        hidden_dim: int=None,  # mlp中间的 hidden_dim\n",
    "\n",
    "        norm_eps: float=1e-5, # RmsNorm\n",
    "        dropout: float=0.0,\n",
    "        \n",
    "        flash_atten: bool=True, # 是否使用flash attention\n",
    "\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.dim = dim\n",
    "        self.n_heads =  n_heads\n",
    "        self.n_kv_heads = n_kv_heads \n",
    "        self.hidden_dim=hidden_dim \n",
    "        self.norm_eps = norm_eps \n",
    "        self.dropout=dropout\n",
    "\n",
    "        self.flash_atten = flash_atten \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.outer?\n",
    "torch.polar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里实现的很神奇  利用的是复数极坐标形式 实现的ROPE\n",
    "def precompute_pos_cis(dim:int, end:int, base:float = 10000.0):\n",
    "    theta = 1.0 / (base **(torch.arange(0,dim,2)[:dim//2].float / dim))\n",
    "    idx = torch.arange(end, device=theta.device)\n",
    "    idx_theta = torch.outer(idx, idx_theta).float() # seq_len, d//2\n",
    "    pos_cis = torch.polar(torch.ones_like(idx_theta),idx_theta) # complex\n",
    "    return pos_cis\n",
    "\n",
    "                   \n",
    "\n",
    "# 旋转位置编码   相对位置 qi,kj = (xi,xj,i-j)\n",
    "def apply_rotary_emb(xq,xk,pos_cis):\n",
    "    # xq  (bs,seq,n_local_heads,head_dim)\n",
    "    def unite_shape(pos_cis, x):\n",
    "        ndim = x.ndim\n",
    "        assert 0<=1<ndim \n",
    "        assert pos_cis.shape == (x.shape[1],x.shape[-1])\n",
    "        shape = [d if i==1 or i==ndim -1 else 1 for i,d in enumerate(x.shape)]\n",
    "        return pos_cis.view(*shape)\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1],-1,2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1],-1,2))\n",
    "    pos_cis = unite_shape(pos_cis, xq_)\n",
    "    xq_out = torch.view_as_real(xq_ * pos_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_ * pos_cis).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.+1.j, 1.+1.j],\n",
       "        [1.+1.j, 1.+1.j]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xq = torch.ones((2,4))\n",
    "print(xq)\n",
    "torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "# torch.view_as_complex?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, args:LMConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        self.n_heads = args.n_heads \n",
    "        assert args.n_heads % args.n_kv_heads ==0 , \"group error\"\n",
    "\n",
    "        self.n_local_heads = self.n_heads\n",
    "        self.n_local_kv_heads = self.n_kv_heads\n",
    "        self.n_rep = self.n_local_heads // self.n_local_kv_heads \n",
    "        self.head_dim = args.dim // args.n_heads #每个head dim为 dim//n_heads\n",
    "\n",
    "        self.wq = nn.Linear(args.dim, self.n_heads * self.head_dim,bias = False)\n",
    "        self.wk = nn.Linear(args.dim, self.n_kv_heads * self.head_dim,bias = False)\n",
    "        self.wv = nn.Linear(args.dim, self.n_kv_heads * self.head_dim,bias = False)\n",
    "        self.wo = nn.Linear(args.dim, args.dim,bias = False)\n",
    "\n",
    "        self.k_cache, self.v_cache = None, None\n",
    "        self.atten_dropout = nn.Dropout(args.dropout)\n",
    "        self.resid_dropout = nn.Dropout(args.dropout) \n",
    "        self.dropout = args.dropout \n",
    "        #  Flash Attention requires PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention') and args.flash_atten \n",
    "\n",
    "        mask = torch.full((1,1,args.max_seq_len,args.max_seq_len),float(\"-inf\"))\n",
    "        mask = torch.triu(mask,diagonal=1)\n",
    "        # buffer中的tensor可以理解为模型的常数\n",
    "        # 只有buffers() 和 parameters()中的属性可以被state_dict保存\n",
    "        # persistent=False， 不需要保存到state_dict中去\n",
    "        self.register_buffer(\"mask\", mask, persistent=False)\n",
    "        \n",
    "    def forward(self,x, poc_cis, kv_cache=False):\n",
    "        # x (bs,sq_len,dim)\n",
    "        bsz, seqlen, _ = x.shape\n",
    "\n",
    "        xq,xk,xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "\n",
    "        # 旋转位置编码  相对位置\n",
    "        xq, xk = apply_rotary_emb(xq,xk,poc_cis)\n",
    "\n",
    "        # kv_cache\n",
    "        if kv_cache and self.eval():\n",
    "            if seqlen == 1 and all(cache is not None for cache in (self.k_vavhe,self.v_cache)):\n",
    "                xk = torch.cat(())\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m\n",
      "\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister_buffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mpersistent\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "Adds a buffer to the module.\n",
      "\n",
      "This is typically used to register a buffer that should not to be\n",
      "considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "is not a parameter, but is part of the module's state. Buffers, by\n",
      "default, are persistent and will be saved alongside parameters. This\n",
      "behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "only difference between a persistent buffer and a non-persistent buffer\n",
      "is that the latter will not be a part of this module's\n",
      ":attr:`state_dict`.\n",
      "\n",
      "Buffers can be accessed as attributes using given names.\n",
      "\n",
      "Args:\n",
      "    name (str): name of the buffer. The buffer can be accessed\n",
      "        from this module using the given name\n",
      "    tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "        that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "        the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "    persistent (bool): whether the buffer is part of this module's\n",
      "        :attr:`state_dict`.\n",
      "\n",
      "Example::\n",
      "\n",
      "    >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "    >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "\u001b[1;31mFile:\u001b[0m      d:\\environment\\anconda5.3.0\\envs\\minimind\\lib\\site-packages\\torch\\nn\\modules\\module.py\n",
      "\u001b[1;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "nn.Module.register_buffer?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
